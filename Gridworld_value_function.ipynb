{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad818c25",
   "metadata": {},
   "source": [
    "# Estimating Value Function in Gridworld Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b93fe",
   "metadata": {},
   "source": [
    "In the Gridworld model, there are 25 states. At each state, there are four available actions and reward is distributed according to the state and action pair.\n",
    "In particular, there are two special states in the model. When one steps onto those two states, the state transition is regardless of the action taken and one gets higher immediate rewards.\n",
    "All the details of the model is presented in the note and we do not mention further details on the setting of Gridworld in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0c20f",
   "metadata": {},
   "source": [
    "## Setting up the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253ba12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from math import isclose\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f3fe85",
   "metadata": {},
   "source": [
    "The state value is formed as a tuple $(i,j)$ where $i,j\\in\\left\\{0,1,2,3,4\\right\\}$. The following function judges if a state tuple is legal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b59df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function judges if the state is legal\n",
    "def is_legal_state(state):\n",
    "    if state[0] < -0.5 or state[0] > 4.5 or state[1] < -0.5 or state[1] > 4.5:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eaf5a0",
   "metadata": {},
   "source": [
    "Given current state and action, the transition kernel decides what the next state is and how much immediate reward an agent receives. The transition kernel is modelled in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e4a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition kernel of MDP\n",
    "# Input: state as a tuple, action as a string\n",
    "# Output: a real number as reward and a tuple as next state\n",
    "def get_reward_state(state,action):\n",
    "    # Two special states\n",
    "    if state == (1,4):\n",
    "        next_state = (1,0)\n",
    "        reward = 10\n",
    "        return reward, next_state\n",
    "    elif state == (3,4):\n",
    "        next_state = (3,2)\n",
    "        reward = 5\n",
    "        return reward, next_state\n",
    "\n",
    "    # General cases\n",
    "    if action == 'N':\n",
    "        next_state = (state[0],state[1] + 1)\n",
    "    elif action == 'S':\n",
    "        next_state = (state[0],state[1] - 1)\n",
    "    elif action == 'W':\n",
    "        next_state = (state[0] - 1,state[1])\n",
    "    elif action == 'E':\n",
    "        next_state = (state[0] + 1,state[1])\n",
    "\n",
    "    # Is the next state legal?\n",
    "    if is_legal_state(next_state):\n",
    "        reward = 0\n",
    "    else:\n",
    "        # Next state illegal, does not move and receive reward -1\n",
    "        next_state = state\n",
    "        reward = -1\n",
    "    return reward, next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3741958",
   "metadata": {},
   "source": [
    "The following function simulates the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fd5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Gridworld until given time limit\n",
    "# Input: simulate until MAX_TIME, the initial state given\n",
    "# Output: lists of state, action, reward as the experience\n",
    "def run_gridworld(MAX_TIME,init_state):\n",
    "    state_list = list()\n",
    "    action_list = list()\n",
    "    reward_list = list()\n",
    "\n",
    "    # Initial state\n",
    "    state = init_state\n",
    "\n",
    "    for t in range(MAX_TIME):\n",
    "        # Figure out the action, reward and next_state\n",
    "        action = action_to_take(state)\n",
    "        reward, next_state = get_reward_state(state,action)\n",
    "\n",
    "        # Record the state,action,reward\n",
    "        state_list.append(state)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "    return state_list,action_list,reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0c80e",
   "metadata": {},
   "source": [
    "The return is calculated through $G_t = R_{t+1} + \\gamma G_{t+1}$ in a backward style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39764f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function computes the return\n",
    "# Input: reward_list, gamma as discount rate\n",
    "# Output: return_list, with length MAX_TIME - 1\n",
    "def compute_return_list(MAX_TIME,reward_list,gamma = 0.9):\n",
    "    return_list = [0] * (MAX_TIME - 1)\n",
    "\n",
    "    # Calculate return backwardly\n",
    "    return_list[-1] = reward_list[-1]\n",
    "    for t in range(MAX_TIME - 3,-1,-1):\n",
    "        return_list[t] = reward_list[t] + gamma * return_list[t + 1]\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee0ba6",
   "metadata": {},
   "source": [
    "## Estimate Value Function for a Given Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914fbff",
   "metadata": {},
   "source": [
    "The policy $\\pi$ is given as the one that always chooses four actions with same probability $\\frac{1}{4}$ regardless of the current state.\n",
    "Let's estimate state value function $v_\\pi$ through Monte Carlo, noticing that it has the structure as an expectation\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi\\left[G_t|S_t = s\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7fce293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return action under policy \\pi\n",
    "def action_to_take(state):\n",
    "    import numpy as np\n",
    "    r = np.random.rand()\n",
    "    if r < 1/4:\n",
    "        return 'N'\n",
    "    elif r < 1/2:\n",
    "        return 'S'\n",
    "    elif r < 3/4:\n",
    "        return 'E'\n",
    "    else:\n",
    "        return 'W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384fed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate v_\\pi through Monte Carlo\n",
    "# Input: NUM_CHAIN as the number of samples in MC\n",
    "# Output: 5 * 5 matrix as the state value function\n",
    "def est_v_pi(NUM_CHAIN,MAX_TIME):\n",
    "\n",
    "    # State value function\n",
    "    v_pi = np.zeros((5,5))\n",
    "\n",
    "    # Store the returns for each state\n",
    "    state_bins = defaultdict(list)\n",
    "\n",
    "    for _ in range(NUM_CHAIN):\n",
    "        # Randomly choose initial state\n",
    "        init_x = np.random.randint(0,5)\n",
    "        init_y = np.random.randint(0,5)\n",
    "        init_state = (init_x,init_y)\n",
    "        \n",
    "        # Simulate\n",
    "        state_list, action_list, reward_list = run_gridworld(MAX_TIME,init_state)\n",
    "\n",
    "        # Compute return\n",
    "        return_list = compute_return_list(MAX_TIME,reward_list)\n",
    "\n",
    "        # If S_t = s, put G_t into the bin of state s\n",
    "        for i in range(len(return_list) - 1,0,-1):\n",
    "            state_bins[state_list[i]].append(return_list[i])\n",
    "\n",
    "    # Compute mean\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            v_pi[i,j] = np.mean(state_bins[(i,j)])\n",
    "\n",
    "    return v_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146636c6",
   "metadata": {},
   "source": [
    "Let's check the estimation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f138eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4\n",
      "0 -1.856261 -0.978066  0.042301  1.521796  3.311155\n",
      "1 -1.344770 -0.435928  0.740045  2.994327  8.790195\n",
      "2 -1.226271 -0.354464  0.675206  2.247773  4.421236\n",
      "3 -1.421055 -0.587406  0.352294  1.902699  5.314808\n",
      "4 -1.976971 -1.188604 -0.409326  0.544080  1.492610\n"
     ]
    }
   ],
   "source": [
    "# Fix random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Parameter\n",
    "NUM_CHAIN = 1000\n",
    "MAX_TIME = 20000\n",
    "\n",
    "# Estimate and print\n",
    "v_pi = est_v_pi(NUM_CHAIN,MAX_TIME)\n",
    "print(pd.DataFrame(data = v_pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ab5f0",
   "metadata": {},
   "source": [
    "## Estimate Optimal Value Function and Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81fdba",
   "metadata": {},
   "source": [
    "The optimal value function $v^*$ is estimated through fixed point iteration thanks to the Bellman optimality operator being a contraction mapping.\n",
    "$$v_{n+1}(s)=\\sup_a \\sum_{s',r}p(s',r|s,a)\\cdot [r+\\gamma\\cdot v_n(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdd880",
   "metadata": {},
   "source": [
    "From $v^*$, we can figure out the optimal policy $\\pi^*$ by noticing that the deterministic optimal policy takes the action that achieves the $\\sup$ in Bellman optimality equation for $v^*(s)$ when the current state is $s$.\n",
    "We integrate the construction of optimal policy in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acb0769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed point iteration to estimate the optimal state value function\n",
    "# and get the optimal policy\n",
    "# Input: error tolerance as stopping criterion is fixed point iteration\n",
    "# Output: 5 * 5 matrix as v_star, a dictionary as optimal policy\n",
    "def est_v_star(TOL,gamma = 0.9):\n",
    "    # Iterate until the estimates are close enough\n",
    "    last_est = np.ones((5,5))\n",
    "    est = np.zeros((5,5))\n",
    "    \n",
    "    # Deterministic optimal policy\n",
    "    opt_policy = dict()\n",
    "\n",
    "    while np.linalg.norm(last_est - est,'fro') > TOL:\n",
    "        last_est = est.copy()\n",
    "        # Fixed point iteration for each state\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                state = (i,j)\n",
    "                # Update the value for state (i,j)\n",
    "                \n",
    "                # Deal with two special states (1,4), (3,4)\n",
    "                if state == (1,4):\n",
    "                    # Next state (1,0) with reward 10\n",
    "                    est[i,j] = 10 + gamma * last_est[1,0]\n",
    "                    opt_policy[state] = 'NSWE'\n",
    "                elif state == (3,4):\n",
    "                    # Next state (3,2) with reward 5\n",
    "                    est[i,j] = 5 + gamma * last_est[3,2]\n",
    "                    opt_policy[state] = 'NSWE'\n",
    "                else:\n",
    "                    # Regular cases\n",
    "                    # four possibilities for next_state \n",
    "                    next_state_list = [(state[0],state[1] + 1),(state[0],state[1] - 1)\n",
    "                                      ,(state[0] - 1,state[1]),(state[0] + 1,state[1])]\n",
    "                    value_list = list()\n",
    "                    for next_state in next_state_list:\n",
    "                        if is_legal_state(next_state):\n",
    "                            # Legal next state, reward 0\n",
    "                            value = gamma * last_est[next_state[0],next_state[1]]\n",
    "                        else:\n",
    "                            # Illegal next state, reward -1\n",
    "                            value = -1 + gamma * last_est[i,j]\n",
    "                        # Append those values to a list\n",
    "                        value_list.append(value)\n",
    "                    # Take the maximum among those four values\n",
    "                    est[i,j] = np.max(value_list)\n",
    "                    \n",
    "                    # Record which action achieves the max\n",
    "                    action_list = ['N','S','W','E']\n",
    "                    best_action = ''\n",
    "                    for k in range(len(action_list)):\n",
    "                            if isclose(est[i,j],value_list[k],abs_tol = 1e-4):\n",
    "                                # Record all best actions for this state\n",
    "                                best_action = best_action + action_list[k]\n",
    "                    # Update the optimal policy\n",
    "                    opt_policy[state] = best_action\n",
    "                    \n",
    "    return est, opt_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817cd8f",
   "metadata": {},
   "source": [
    "Let's derive the estimate for $v^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faee1446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0          1          2          3          4\n",
      "0  14.419349  16.021499  17.801666  19.779673  21.977414\n",
      "1  16.021499  17.801666  19.779673  21.977414  24.419349\n",
      "2  14.419349  16.021499  17.801666  19.779673  21.977414\n",
      "3  12.977414  14.419349  16.021499  17.801666  19.419349\n",
      "4  11.679673  12.977414  14.419349  16.021499  17.477414\n"
     ]
    }
   ],
   "source": [
    "TOL = 1e-4\n",
    "v_star, opt_policy = est_v_star(TOL)\n",
    "print(pd.DataFrame(data = v_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d1f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0     1\n",
      "0   (0, 0)    NE\n",
      "1   (0, 1)    NE\n",
      "2   (0, 2)    NE\n",
      "3   (0, 3)    NE\n",
      "4   (0, 4)     E\n",
      "5   (1, 0)     N\n",
      "6   (1, 1)     N\n",
      "7   (1, 2)     N\n",
      "8   (1, 3)     N\n",
      "9   (1, 4)  NSWE\n",
      "10  (2, 0)    NW\n",
      "11  (2, 1)    NW\n",
      "12  (2, 2)    NW\n",
      "13  (2, 3)    NW\n",
      "14  (2, 4)     W\n",
      "15  (3, 0)    NW\n",
      "16  (3, 1)    NW\n",
      "17  (3, 2)    NW\n",
      "18  (3, 3)     W\n",
      "19  (3, 4)  NSWE\n",
      "20  (4, 0)    NW\n",
      "21  (4, 1)    NW\n",
      "22  (4, 2)    NW\n",
      "23  (4, 3)     W\n",
      "24  (4, 4)     W\n"
     ]
    }
   ],
   "source": [
    "# Print optimal policy\n",
    "print(pd.DataFrame(data = opt_policy.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d7250",
   "metadata": {},
   "source": [
    "In this notebook, we shown how to compute the value function for a specific policy from the experience of the game (model-free), and how to estimate the optimal value function, optimal policy on knowing the transition kernel (model-based).\n",
    "Those are the fundamentals of the RL algorithm we will build in a later context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7a034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
